conda create --name diff_rl python==3.10.13

# Start from here
conda install pytorch==2.2.0 torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
pip install stable-baselines3==2.2.1
conda install -c conda-forge libstdcxx-ng (for libGL)
pip install swig (for gym box2d)
pip install mujoco==2.3.7
pip install gymnasium==0.29.1
pip install numpy==1.26.3

gedit .bashrc	
    add:
    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia

pip install psutil
pip install tensorboard>=2.9.1
cd envs --> pip install -e .

----------------------------------------------------

推导：

定义：
L = α * log π(a|μ,σ) - Q(a)
其中：
log π(a|μ,σ) = -0.5 * log(2π) - log σ - (a - μ)² / (2σ²)
a = μ + σ * ε，ε ~ N(0,1)

1️⃣ 对 μ 的推导：

先求：
∂log π/∂μ
log π 中只有 - (a - μ)² / (2σ²) 项与 μ 有关：
∂/∂μ [-(a - μ)² / (2σ²)] = (a - μ) / σ²

而 a 对 μ 的导数：
a = μ + σ ε，∂a/∂μ = 1

Q(a) 对 μ：
∂Q/∂μ = Q'(a) * ∂a/∂μ = Q'(a)

因为 L 中 Q 项是负的：
∂L/∂μ = α * ∂log π/∂μ - ∂Q/∂μ
∂L/∂μ = α * (a - μ) / σ² - Q'(a) 但是由于rsample, 这一部分导数为0

2️⃣ 对 σ 的推导：

先求：
∂log π/∂σ
log π 中包含 -log σ 和 (a - μ)² / (2σ²)：
∂/∂σ [-log σ] = -1/σ
∂/∂σ [-(a - μ)² / (2σ²)] = (a - μ)² / σ³

a 对 σ 的导数：
a = μ + σ ε，∂a/∂σ = ε

Q(a) 对 σ：
∂Q/∂σ = Q'(a) * ∂a/∂σ = Q'(a) * ε

因此：
∂L/∂σ = α * (-1/σ + (a - μ)² / σ³) - Q'(a) * ε

3️⃣ 对 log σ 的推导：

链式法则：
∂L/∂log σ = ∂L/∂σ * ∂σ/∂log σ
因为：
σ = e^{log σ}，∂σ/∂log σ = σ

因此：
∂L/∂log σ = σ * [α * (-1/σ + (a - μ)² / σ³) - Q'(a) * ε]

最终结果：
∂L/∂μ = α * (a - μ) / σ² - Q'(a)
∂L/∂σ = α * (-1/σ + (a - μ)² / σ³) - Q'(a) * ε
∂L/∂log σ = σ * [α * (-1/σ + (a - μ)² / σ³) - Q'(a) * ε]

--------------------------------------------------------------

OffPolicyAlgorithm:
	unscaled_action, _ = self.predict(self._last_obs, deterministic=False)

--> BaseAlgorithm:
	return self.policy.predict(observation, state, episode_start, deterministic)
	
--> TD3Policy:
    def _predict(self, observation, deterministic: bool = False):
        action = self.actor(observation)
        return action

--> BasePolicy:
    def predict(
        self,
        observation: Union[np.ndarray, Dict[str, np.ndarray]],
        state: Optional[Tuple[np.ndarray, ...]] = None,
        episode_start: Optional[np.ndarray] = None,
        deterministic: bool = False,
    ):

        # Switch to eval mode (this affects batch norm / dropout)
        self.set_training_mode(False)

        obs_tensor, vectorized_env = self.obs_to_tensor(observation)

        with th.no_grad(): 
            actions = self._predict(obs_tensor, deterministic=deterministic) ########## This is where get actor output
        # Convert to numpy, and reshape to the original action shape
        actions = actions.cpu().numpy().reshape((-1, *self.action_space.shape))  # type: ignore[misc]

        if isinstance(self.action_space, spaces.Box):
            if self.squash_output:
                # Rescale to proper domain when using squashing
                actions = self.unscale_action(actions)  # type: ignore[assignment, arg-type]
            else: # Rescale the action from [-1, 1] to [low, high]
                # Actions could be on arbitrary scale, so clip the actions to avoid
                # out of bound error (e.g. if sampling from a Gaussian distribution)
                actions = np.clip(actions, self.action_space.low, self.action_space.high)  # type: ignore[assignment, arg-type]

        # Remove batch dimension if needed
        if not vectorized_env:
            assert isinstance(actions, np.ndarray)
            actions = actions.squeeze(axis=0)

        return actions, state 
	